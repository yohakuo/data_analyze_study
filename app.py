import json
import logging

import pandas as pd
import streamlit as st
import yaml

from src.features.calculator import FeatureCalculator
from src.features.llm import HybridLLMService
from src.io import load_heritage_data

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(page_title="ç›‘æµ‹æ•°æ®ç®¡ç†", layout="wide")

# --- ä¾§è¾¹æ ï¼šé…ç½® ---
st.sidebar.title("æ™ºèƒ½ä½“é…ç½®")
model_source = st.sidebar.radio("é€‰æ‹©æ¨¡å‹æ¥æº", ["Local (æœ¬åœ° Ollama)", "Online (OpenAI Compatible)"])

if model_source == "Local (æœ¬åœ° Ollama)":
    mode = "local"
    base_url = st.sidebar.text_input("Local API URL", "http://localhost:11434/v1")
    api_key = "ollama"
    model_name = st.sidebar.text_input("æ¨¡å‹åç§°", "qwen2.5:7b")
else:
    mode = "online"
    base_url = st.sidebar.text_input(
        "Online API URL", "https://dashscope.aliyuncs.com/compatible-mode/v1"
    )
    api_key = st.sidebar.text_input("API Key", type="password")
    model_name = st.sidebar.text_input("æ¨¡å‹åç§°", "qwen-plus")


# --- åŠ è½½èµ„æº ---
@st.cache_resource
def get_calculator():
    return FeatureCalculator()


@st.cache_data
def get_data():
    try:
        return load_heritage_data("data")  # è¯»å– data/ æ–‡ä»¶å¤¹
    except Exception as e:
        logger.error(f"Error loading data: {e}")
        return pd.DataFrame()


# åˆå§‹åŒ–æœåŠ¡
try:
    calculator = get_calculator()
    df = get_data()
    # Initialize LLM service with current config
    # Note: We don't cache this resource to allow dynamic config changes, 
    # but in production you might want to cache if initialization is heavy.
    llm_service = HybridLLMService(
        mode=mode, 
        base_url=base_url, 
        api_key=api_key, 
        model_name=model_name
    )
except Exception as e:
    st.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
    st.stop()

if df.empty:
    st.warning("data/ ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ° CSV æ•°æ®ï¼Œè¯·æ”¾å…¥æ•°æ®ååˆ·æ–°ã€‚")
    st.stop()


# --- ä¸šåŠ¡é€»è¾‘ ---
def execute_analysis(calculator, df, metric_def, filter_params):
    """
    æ ¹æ®æŒ‡æ ‡å®šä¹‰å’Œè¿‡æ»¤å‚æ•°æ‰§è¡Œåˆ†æ
    """
    # 1. åŠ¨æ€è¿‡æ»¤
    filtered_df = df.copy()

    # ç­›é€‰ç‚¹ä½ (ç¤ºä¾‹ logic)
    if "site_id" in filter_params and filter_params["site_id"]:
        if "site_id" in filtered_df.columns:
            filtered_df = filtered_df[filtered_df["site_id"] == filter_params["site_id"]]

    # ç­›é€‰æ—¶é—´
    try:
        if "start_date" in filter_params and filter_params["start_date"]:
             filtered_df = filtered_df[filtered_df.index >= filter_params["start_date"]]
        
        if "end_date" in filter_params and filter_params["end_date"]:
             filtered_df = filtered_df[filtered_df.index <= filter_params["end_date"]]
    except Exception as e:
        logger.warning(f"Error filtering by date: {e}")

    if filtered_df.empty:
        return None, "è¿‡æ»¤åæ— æ•°æ®"

    # 2. å‡†å¤‡è®¡ç®—å‚æ•°
    field = metric_def["data_field"]
    features = metric_def["calculation_logic"]
    # ä¼˜å…ˆç”¨LLMæå–çš„é¢‘ç‡ï¼Œå¦åˆ™ç”¨é»˜è®¤
    freq = filter_params.get("freq") or metric_def["default_freq"]

    # 3. æ‰§è¡Œè®¡ç®—
    try:
        result_df = calculator.calculate_statistical_features(
            filtered_df, field_name=field, feature_list=features, freq=freq
        )
        return result_df, None
    except Exception as e:
        return None, f"è®¡ç®—é”™è¯¯: {e}"


# --- ç•Œé¢äº¤äº’åŒºåŸŸ ---
st.title(" ç¯å¢ƒç›‘æµ‹è¯­ä¹‰åˆ†æç³»ç»Ÿ")
st.caption(f"å½“å‰æ¥å…¥æ•°æ®ï¼š{len(df)} æ¡ | æ—¶é—´èŒƒå›´ï¼š{df.index.min()} è‡³ {df.index.max()}")

# èŠå¤©è¾“å…¥
user_query = st.chat_input(
    "è¯·è¾“å…¥æŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼š'å¸®æˆ‘çœ‹çœ‹æ¸©åº¦çš„å˜åŒ–è¶‹åŠ¿' æˆ– 'åˆ†æä¸€ä¸‹æ¹¿åº¦çš„æ¯æ—¥ç»Ÿè®¡'"
)

if user_query:
    # 4.1 æ˜¾ç¤ºç”¨æˆ·æé—®
    with st.chat_message("user"):
        st.write(user_query)

    # 4.2 LLM è¯­ä¹‰åˆ†æ
    with st.chat_message("assistant"):
        with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
            intent = llm_service.parse_intent(user_query)
        
        if "error" in intent:
             st.error(f"æ„å›¾è¯†åˆ«å¤±è´¥: {intent['error']}")
             st.stop()
             
        metric_id = intent.get("metric_id")
        params = intent.get("params", {})
        
        if metric_id:
            # æŸ¥æ‰¾æŒ‡æ ‡å®šä¹‰
            metric_config = next(
                (m for m in llm_service.metrics_def["metrics"] if m["id"] == metric_id), 
                None
            )
            
            if metric_config:
                st.success(f"å·²è¯†åˆ«æ„å›¾ï¼š**{metric_config['name']}**")
                with st.expander("æŸ¥çœ‹è§£æå‚æ•°"):
                    st.json(params)
                
                # æ‰§è¡Œåˆ†æ
                result_df, error_msg = execute_analysis(calculator, df, metric_config, params)
                
                if error_msg:
                    st.warning(error_msg)
                else:
                    st.subheader("ğŸ“Š åˆ†æå›¾è¡¨")
                    
                    # ç®€å•å¤„ç† MultiIndex åˆ—åä»¥ä¾¿ç»˜å›¾
                    if isinstance(result_df.columns, pd.MultiIndex):
                        result_df.columns = [
                            "_".join(col).strip() for col in result_df.columns.values
                        ]
                    
                    viz_type = metric_config.get("viz_type", "line")
                    if viz_type == "line":
                        st.line_chart(result_df)
                    elif viz_type == "area":
                        st.area_chart(result_df)
                    elif viz_type == "bar":
                        st.bar_chart(result_df)
                    else:
                        st.line_chart(result_df)

                    with st.expander("æŸ¥çœ‹è¯¦ç»†æ•°æ®"):
                        st.dataframe(result_df)
            else:
                st.error(f"æœªæ‰¾åˆ°æŒ‡æ ‡å®šä¹‰: {metric_id}")
        else:
            st.warning("æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„æ„å›¾ï¼Œæˆ–è€…è¯¥åˆ†æå°šä¸æ”¯æŒã€‚")
