import datetime
import json
import sys
import time
from zoneinfo import ZoneInfo

import numpy as np
import openai
from openai import OpenAI
import pandas as pd

# ä»æˆ‘ä»¬è‡ªå·±çš„æ¨¡å—ä¸­å¯¼å…¥æ‰€æœ‰å·¥å…·å’Œé…ç½®
from src import config
from src.dataset import get_timeseries_data
from src.features import calculate_hourly_features, recalculate_single_hour_features
from src.tools import TOOL_LIST


def find_canonical_field_name(user_term: str) -> str | None:
    """
    æ ¹æ®ç”¨æˆ·è¾“å…¥çš„è¯è¯­ï¼Œåœ¨ config çš„åˆ«åè¡¨ä¸­æŸ¥æ‰¾å¹¶è¿”å›è§„èŒƒçš„å­—æ®µåã€‚
    """
    # 1. å…ˆæ£€æŸ¥ç”¨æˆ·è¾“å…¥çš„è¯æœ¬èº«æ˜¯ä¸æ˜¯å°±æ˜¯ä¸€ä¸ªè§„èŒƒå
    if user_term in config.FIELD_ALIAS_MAP:
        return user_term

    # 2. éå†åˆ«åè¡¨ï¼ŒæŸ¥æ‰¾åŒ¹é…çš„åˆ«å
    for canonical_name, aliases in config.FIELD_ALIAS_MAP.items():
        if user_term in aliases:
            print(f"  [ç¿»è¯‘å®˜]ï¼šå·²å°†ç”¨æˆ·è¾“å…¥çš„ '{user_term}' ç¿»è¯‘ä¸ºè§„èŒƒå­—æ®µå '{canonical_name}'")
            return canonical_name

    # 3. å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å› None
    print(f"  [ç¿»è¯‘å®˜]ï¼šè­¦å‘Šï¼åœ¨åˆ«åè¡¨ä¸­æ‰¾ä¸åˆ°ä¸ '{user_term}' åŒ¹é…çš„å­—æ®µã€‚")
    return None


def create_local_llm_client():
    try:
        client = OpenAI(base_url="http://192.168.121.57:11434/v1", api_key="ollama")
        print("æˆåŠŸåˆ›å»º API å®¢æˆ·ç«¯ï¼Œå·²æŒ‡å‘æœ¬åœ° Ollama æœåŠ¡ã€‚")
        return client
    except Exception as e:
        print(f"âŒ åˆ›å»º API å®¢æˆ·ç«¯å¤±è´¥: {e}")
        sys.exit(1)


def execute_tool_call(tool_call):
    """
    æ¥æ”¶ LLM è¿”å›çš„å·¥å…·è°ƒç”¨æŒ‡ä»¤ï¼Œå¹¶åœ¨ Python ä¸­æ‰§è¡Œå¯¹åº”çš„å‡½æ•°ã€‚
    """
    function_name = tool_call.function.name
    function_args = json.loads(tool_call.function.arguments)

    print(f"ğŸ¤– LLM å†³å®šè°ƒç”¨å‡½æ•°: {function_name}ï¼ŒåŸå§‹å‚æ•°: {function_args}")

    if function_name == "calculate_hourly_features":
        user_field_name = function_args.get("field_name")
        canonical_field_name = find_canonical_field_name(user_field_name)

        if not canonical_field_name:
            return {"error": f"æ— æ³•è¯†åˆ«çš„å­—æ®µå '{user_field_name}'ã€‚"}

        features_to_calculate = function_args.get("features_to_calculate")
        start_time_str = function_args.get("start_time")
        end_time_str = function_args.get("end_time")

        local_tz = ZoneInfo(config.LOCAL_TIMEZONE)
        start_time_dt = (
            datetime.datetime.strptime(start_time_str, "%Y-%m-%d").replace(tzinfo=local_tz)
            if start_time_str
            else None
        )
        end_time_dt = (
            datetime.datetime.strptime(end_time_str, "%Y-%m-%d").replace(tzinfo=local_tz)
            if end_time_str
            else None
        )

        raw_df = get_timeseries_data(
            measurement_name=config.INFLUX_MEASUREMENT_NAME,
            field_name=canonical_field_name,
            start_time=start_time_dt,
            stop_time=end_time_dt,
        )

        features_df = calculate_hourly_features(
            raw_df, canonical_field_name, features_to_calculate
        )

        return features_df.to_string(max_rows=5, max_cols=20)

    else:
        return {"error": "æœªæ‰¾åˆ°å¯¹åº”çš„å‡½æ•°"}


def main():
    client = create_local_llm_client()

    while True:
        user_message = input("\nè¯·æå‡ºä½ çš„æ•°æ®åˆ†æè¯·æ±‚ (è¾“å…¥ 'exit' é€€å‡º): ")
        if user_message.lower() == "exit":
            break

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†æ•°æ®åˆ†æä»»åŠ¡çš„å·¥å…·ä»£ç†ã€‚"},
            {"role": "user", "content": user_message},
        ]

        # å°†ç”¨æˆ·è¯·æ±‚å’Œå·¥å…·æ¸…å•å‘ç»™ LLM
        response = client.chat.completions.create(
            model="qwen3:8b",
            messages=messages,
            tools=TOOL_LIST,
            tool_choice="auto",
        )
        response_message = response.choices[0].message

        # æ£€æŸ¥ LLM æ˜¯å¦å†³å®šè°ƒç”¨å·¥å…·
        if response_message.tool_calls:
            tool_call = response_message.tool_calls[0]
            function_response = execute_tool_call(tool_call)

            # å°†å·¥å…·çš„æ‰§è¡Œç»“æœè¿”å›ç»™ LLM
            messages.append(response_message)  # æ·»åŠ  LLM çš„å·¥å…·è°ƒç”¨æŒ‡ä»¤
            messages.append(
                {
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": tool_call.function.name,
                    "content": function_response,
                }
            )

            # å†æ¬¡è°ƒç”¨ LLMï¼Œè®©å®ƒæ ¹æ®å·¥å…·çš„æ‰§è¡Œç»“æœè¿›è¡Œæ€»ç»“
            second_response = client.chat.completions.create(model="qwen3:8b", messages=messages)
            print("\næ¨¡å‹æ€»ç»“ï¼š")
            print(second_response.choices[0].message.content)

        else:
            print("\næ¨¡å‹å›å¤ï¼š")
            print(response_message.content)


if __name__ == "__main__":
    main()
